{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial_YoLo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BvrEuH5Rw545"},"source":["#Object Detection With YOLOv3\n","\n","* The keras-yolo3 project provides a lot of capability for using YOLOv3 models, including object detection, transfer learning, and training new models from scratch.\n","* In this section, we will use a pre-trained model to perform object detection on an unseen photograph.\n","\n","\n","\n","\n","---\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bb2xnrAX0Fhd"},"source":["#Create and Save Model\n","\n","\n","* The first step is to download the pre-trained model weights.\n","YOLOv3 Pre-trained Model Weights (yolov3.weights) (237 MB)\n","[link text](https://pjreddie.com/media/files/yolov3.weights)\n","*   These were trained using the DarkNet code base on the MSCOCO dataset.\n","*   Download the model weights and place them into your current working directory with the filename “yolov3.weights.”\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHcm2anXBHEK","executionInfo":{"status":"ok","timestamp":1608014899951,"user_tz":-60,"elapsed":992,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}},"outputId":"1f47ed05-1d21-41d5-9cee-3de9d4636728"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RYDLkIKp4235","executionInfo":{"status":"ok","timestamp":1608014943323,"user_tz":-60,"elapsed":1317,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["# create a YOLOv3 Keras model and save it to file\n","# based on https://github.com/experiencor/keras-yolo3\n","import struct\n","import numpy as np\n","from keras.layers import Conv2D\n","from keras.layers import Input\n","from keras.layers import BatchNormalization\n","from keras.layers import LeakyReLU\n","from keras.layers import ZeroPadding2D\n","from keras.layers import UpSampling2D\n","from keras.layers.merge import add, concatenate\n","from keras.models import Model\n","\n","def _conv_block(inp, convs, skip=True):\n","\tx = inp\n","\tcount = 0\n","\tfor conv in convs:\n","\t\tif count == (len(convs) - 2) and skip:\n","\t\t\tskip_connection = x\n","\t\tcount += 1\n","\t\tif conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n","\t\tx = Conv2D(conv['filter'],\n","\t\t\t\t   conv['kernel'],\n","\t\t\t\t   strides=conv['stride'],\n","\t\t\t\t   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n","\t\t\t\t   name='conv_' + str(conv['layer_idx']),\n","\t\t\t\t   use_bias=False if conv['bnorm'] else True)(x)\n","\t\tif conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n","\t\tif conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n","\treturn add([skip_connection, x]) if skip else x\n","\n","def make_yolov3_model():\n","\tinput_image = Input(shape=(None, None, 3))\n","\t# Layer  0 => 4\n","\tx = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n","\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n","\t\t\t\t\t\t\t\t  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n","\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n","\t# Layer  5 => 8\n","\tx = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n","\t\t\t\t\t\t{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n","\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n","\t# Layer  9 => 11\n","\tx = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n","\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n","\t# Layer 12 => 15\n","\tx = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n","\t\t\t\t\t\t{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n","\t# Layer 16 => 36\n","\tfor i in range(7):\n","\t\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n","\t\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n","\tskip_36 = x\n","\t# Layer 37 => 40\n","\tx = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n","\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n","\t# Layer 41 => 61\n","\tfor i in range(7):\n","\t\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n","\t\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n","\tskip_61 = x\n","\t# Layer 62 => 65\n","\tx = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n","\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n","\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n","\t# Layer 66 => 74\n","\tfor i in range(3):\n","\t\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n","\t\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n","\t# Layer 75 => 79\n","\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n","\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n","\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n","\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n","\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n","\t# Layer 80 => 82\n","\tyolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n","\t\t\t\t\t\t\t  {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n","\t# Layer 83 => 86\n","\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n","\tx = UpSampling2D(2)(x)\n","\tx = concatenate([x, skip_61])\n","\t# Layer 87 => 91\n","\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n","\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n","\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n","\t# Layer 92 => 94\n","\tyolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n","\t\t\t\t\t\t\t  {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n","\t# Layer 95 => 98\n","\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n","\tx = UpSampling2D(2)(x)\n","\tx = concatenate([x, skip_36])\n","\t# Layer 99 => 106\n","\tyolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n","\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n","\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n","\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n","\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n","\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n","\t\t\t\t\t\t\t   {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n","\tmodel = Model(input_image, [yolo_82, yolo_94, yolo_106])\n","\treturn model\n","\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oI83b4HcLwCV"},"source":["## Load the model weights.\r\n","\r\n","\r\n","*  The model weights are stored in whatever format that was used by DarkNet. Rather than trying to decode the file manually, we can use the WeightReader class provided in the script.\r\n","\r\n","*  To use the WeightReader, it is instantiated with the path to our weights file (e.g. ‘yolov3.weights‘). This will parse the file and load the model weights into memory in a format that we can set into our Keras model.\r\n","\r\n"," "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":409},"id":"1KxLIyIDLpuI","executionInfo":{"status":"error","timestamp":1608014903136,"user_tz":-60,"elapsed":4155,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}},"outputId":"04ab3c1d-acf5-4db8-af5c-a51c402d9372"},"source":["class WeightReader:\r\n","\tdef __init__(self, weight_file):\r\n","\t\twith open(weight_file, 'rb') as w_f:\r\n","\t\t\tmajor,\t= struct.unpack('i', w_f.read(4))\r\n","\t\t\tminor,\t= struct.unpack('i', w_f.read(4))\r\n","\t\t\trevision, = struct.unpack('i', w_f.read(4))\r\n","\t\t\tif (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\r\n","\t\t\t\tw_f.read(8)\r\n","\t\t\telse:\r\n","\t\t\t\tw_f.read(4)\r\n","\t\t\ttranspose = (major > 1000) or (minor > 1000)\r\n","\t\t\tbinary = w_f.read()\r\n","\t\tself.offset = 0\r\n","\t\tself.all_weights = np.frombuffer(binary, dtype='float32')\r\n","\r\n","\tdef read_bytes(self, size):\r\n","\t\tself.offset = self.offset + size\r\n","\t\treturn self.all_weights[self.offset-size:self.offset]\r\n","\r\n","\tdef load_weights(self, model):\r\n","\t\tfor i in range(106):\r\n","\t\t\ttry:\r\n","\t\t\t\tconv_layer = model.get_layer('conv_' + str(i))\r\n","\t\t\t\tprint(\"loading weights of convolution #\" + str(i))\r\n","\t\t\t\tif i not in [81, 93, 105]:\r\n","\t\t\t\t\tnorm_layer = model.get_layer('bnorm_' + str(i))\r\n","\t\t\t\t\tsize = np.prod(norm_layer.get_weights()[0].shape)\r\n","\t\t\t\t\tbeta  = self.read_bytes(size) # bias\r\n","\t\t\t\t\tgamma = self.read_bytes(size) # scale\r\n","\t\t\t\t\tmean  = self.read_bytes(size) # mean\r\n","\t\t\t\t\tvar   = self.read_bytes(size) # variance\r\n","\t\t\t\t\tweights = norm_layer.set_weights([gamma, beta, mean, var])\r\n","\t\t\t\tif len(conv_layer.get_weights()) > 1:\r\n","\t\t\t\t\tbias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\r\n","\t\t\t\t\tkernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\r\n","\t\t\t\t\tkernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\r\n","\t\t\t\t\tkernel = kernel.transpose([2,3,1,0])\r\n","\t\t\t\t\tconv_layer.set_weights([kernel, bias])\r\n","\t\t\t\telse:\r\n","\t\t\t\t\tkernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\r\n","\t\t\t\t\tkernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\r\n","\t\t\t\t\tkernel = kernel.transpose([2,3,1,0])\r\n","\t\t\t\t\tconv_layer.set_weights([kernel])\r\n","\t\t\texcept ValueError:\r\n","\t\t\t\tprint(\"no convolution #\" + str(i))\r\n","\r\n","\tdef reset(self):\r\n","\t\tself.offset = 0\r\n","\r\n","# define the model\r\n","model = make_yolov3_model()\r\n","# load the model weights\r\n","#f='/content/drive/MyDrive/Colab Notebooks/yolov3.weights'\r\n","f='/content/drive/MyDrive/__Master Computational Vision 2020-2021/theory/tutorial yolo/yolov3.weights'\r\n","weight_reader = WeightReader(f)\r\n","# set the model weights into the model\r\n","weight_reader.load_weights(model)\r\n","# save the model to file\r\n","model.save('model.h5')"],"execution_count":7,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-ecc9c40a9482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#f='/content/drive/MyDrive/Colab Notebooks/yolov3.weights'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/__Master Computational Vision 2020-2021/theory/tutorial yolo/yolov3.weights'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mweight_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeightReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;31m# set the model weights into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mweight_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-ecc9c40a9482>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWeightReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mw_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                         \u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mminor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/__Master Computational Vision 2020-2021/theory/tutorial yolo/yolov3.weights'"]}]},{"cell_type":"markdown","metadata":{"id":"mYz22udj2GUE"},"source":["#Make a Prediction\n","\n","\n","*   We need a new photo for object detection, ideally with objects that we know that the model knows about from the [MSCOCO dataset](https://cocodataset.org/#home).\n","*   We will use a photograph of three zebras taken by Boegh on safari, and released under a permissive license.\n","*   Download the photograph and place it in your current working directory with the filename ‘[zebra.jpg](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/03/zebra.jpg)‘\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1H78q13aQ5g9"},"source":["# Input Image"]},{"cell_type":"code","metadata":{"id":"o7FhPO7ZRkan","executionInfo":{"status":"aborted","timestamp":1608014903115,"user_tz":-60,"elapsed":4123,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["import matplotlib.pyplot as plt\r\n","from PIL import Image\r\n","\r\n","img = Image.open('/content/drive/MyDrive/__Master Computational Vision 2020-2021/theory/tutorial yolo/zebra.jpg')\r\n","img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQVGELej7vLg","executionInfo":{"status":"aborted","timestamp":1608014903117,"user_tz":-60,"elapsed":4117,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["# load yolov3 model and perform object detection\n","# based on https://github.com/experiencor/keras-yolo3\n","import numpy as np\n","from numpy import expand_dims\n","from keras.models import load_model\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from matplotlib import pyplot\n","from matplotlib.patches import Rectangle\n","\n","class BoundBox:\n","\tdef __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n","\t\tself.xmin = xmin\n","\t\tself.ymin = ymin\n","\t\tself.xmax = xmax\n","\t\tself.ymax = ymax\n","\t\tself.objness = objness\n","\t\tself.classes = classes\n","\t\tself.label = -1\n","\t\tself.score = -1\n","\n","\tdef get_label(self):\n","\t\tif self.label == -1:\n","\t\t\tself.label = np.argmax(self.classes)\n","\n","\t\treturn self.label\n","\n","\tdef get_score(self):\n","\t\tif self.score == -1:\n","\t\t\tself.score = self.classes[self.get_label()]\n","\n","\t\treturn self.score\n","\n","def _sigmoid(x):\n","\treturn 1. / (1. + np.exp(-x))\n","\n","def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n","\tgrid_h, grid_w = netout.shape[:2]\n","\tnb_box = 3\n","\tnetout = netout.reshape((grid_h, grid_w, nb_box, -1))\n","\tnb_class = netout.shape[-1] - 5\n","\tboxes = []\n","\tnetout[..., :2]  = _sigmoid(netout[..., :2])\n","\tnetout[..., 4:]  = _sigmoid(netout[..., 4:])\n","\tnetout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n","\tnetout[..., 5:] *= netout[..., 5:] > obj_thresh\n","\n","\tfor i in range(grid_h*grid_w):\n","\t\trow = i / grid_w\n","\t\tcol = i % grid_w\n","\t\tfor b in range(nb_box):\n","\t\t\t# 4th element is objectness score\n","\t\t\tobjectness = netout[int(row)][int(col)][b][4]\n","\t\t\tif(objectness.all() <= obj_thresh): continue\n","\t\t\t# first 4 elements are x, y, w, and h\n","\t\t\tx, y, w, h = netout[int(row)][int(col)][b][:4]\n","\t\t\tx = (col + x) / grid_w # center position, unit: image width\n","\t\t\ty = (row + y) / grid_h # center position, unit: image height\n","\t\t\tw = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n","\t\t\th = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n","\t\t\t# last elements are class probabilities\n","\t\t\tclasses = netout[int(row)][col][b][5:]\n","\t\t\tbox = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n","\t\t\tboxes.append(box)\n","\treturn boxes\n","\n","def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n","\tnew_w, new_h = net_w, net_h\n","\tfor i in range(len(boxes)):\n","\t\tx_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n","\t\ty_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n","\t\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n","\t\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n","\t\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n","\t\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n","\n","def _interval_overlap(interval_a, interval_b):\n","\tx1, x2 = interval_a\n","\tx3, x4 = interval_b\n","\tif x3 < x1:\n","\t\tif x4 < x1:\n","\t\t\treturn 0\n","\t\telse:\n","\t\t\treturn min(x2,x4) - x1\n","\telse:\n","\t\tif x2 < x3:\n","\t\t\t return 0\n","\t\telse:\n","\t\t\treturn min(x2,x4) - x3\n","\n","def bbox_iou(box1, box2):\n","\tintersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n","\tintersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n","\tintersect = intersect_w * intersect_h\n","\tw1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n","\tw2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n","\tunion = w1*h1 + w2*h2 - intersect\n","\treturn float(intersect) / union\n","\n","def do_nms(boxes, nms_thresh):\n","\tif len(boxes) > 0:\n","\t\tnb_class = len(boxes[0].classes)\n","\telse:\n","\t\treturn\n","\tfor c in range(nb_class):\n","\t\tsorted_indices = np.argsort([-box.classes[c] for box in boxes])\n","\t\tfor i in range(len(sorted_indices)):\n","\t\t\tindex_i = sorted_indices[i]\n","\t\t\tif boxes[index_i].classes[c] == 0: continue\n","\t\t\tfor j in range(i+1, len(sorted_indices)):\n","\t\t\t\tindex_j = sorted_indices[j]\n","\t\t\t\tif bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n","\t\t\t\t\tboxes[index_j].classes[c] = 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oN5_bAOThA6a"},"source":["do_nms() function that takes the list of bounding boxes and a threshold parameter. Rather than purging the overlapping boxes, their predicted probability for their overlapping class is cleared. This allows the boxes to remain and be used if they also detect another object type."]},{"cell_type":"code","metadata":{"id":"Kr2vbc-qhG-O","executionInfo":{"status":"aborted","timestamp":1608014903119,"user_tz":-60,"elapsed":4107,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["def do_nms(boxes, nms_thresh):\r\n","\tif len(boxes) > 0:\r\n","\t\tnb_class = len(boxes[0].classes)\r\n","\telse:\r\n","\t\treturn\r\n","\tfor c in range(nb_class):\r\n","\t\tsorted_indices = np.argsort([-box.classes[c] for box in boxes])\r\n","\t\tfor i in range(len(sorted_indices)):\r\n","\t\t\tindex_i = sorted_indices[i]\r\n","\t\t\tif boxes[index_i].classes[c] == 0: continue\r\n","\t\t\tfor j in range(i+1, len(sorted_indices)):\r\n","\t\t\t\tindex_j = sorted_indices[j]\r\n","\t\t\t\tif bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\r\n","\t\t\t\t\tboxes[index_j].classes[c] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3X1MBXlYZKah"},"source":["# load and prepare an image"]},{"cell_type":"code","metadata":{"id":"0uG5kG59WnpC","executionInfo":{"status":"aborted","timestamp":1608014903120,"user_tz":-60,"elapsed":4100,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["\r\n","def load_image_pixels(filename, shape):\r\n","\t# load the image to get its shape\r\n","\timage = load_img(filename)\r\n","\twidth, height = image.size\r\n","\t# load the image with the required size\r\n","\timage = load_img(filename, target_size=shape)\r\n","\t# convert to numpy array\r\n","\timage = img_to_array(image)\r\n","\t# scale pixel values to [0, 1]\r\n","\timage = image.astype('float32')\r\n","\timage /= 255.0\r\n","\t# add a dimension so that we have one sample\r\n","\timage = expand_dims(image, 0)\r\n","\treturn image, width, height"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oT4HhBonWzgI","executionInfo":{"status":"aborted","timestamp":1608014903122,"user_tz":-60,"elapsed":4097,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["# get all of the results above a threshold\r\n","def get_boxes(boxes, labels, thresh):\r\n","\tv_boxes, v_labels, v_scores = list(), list(), list()\r\n","\t# enumerate all boxes\r\n","\tfor box in boxes:\r\n","\t\t# enumerate all possible labels\r\n","\t\tfor i in range(len(labels)):\r\n","\t\t\t# check if the threshold for this label is high enough\r\n","\t\t\tif box.classes[i] > thresh:\r\n","\t\t\t\tv_boxes.append(box)\r\n","\t\t\t\tv_labels.append(labels[i])\r\n","\t\t\t\tv_scores.append(box.classes[i]*100)\r\n","\t\t\t\t# don't break, many labels may trigger for one box\r\n","\treturn v_boxes, v_labels, v_scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N3EdW-UAeKE2"},"source":["# draw all results\r\n","The draw_boxes() function below implements this, taking the filename of the original photograph and the parallel lists of bounding boxes, labels and scores, and creates a plot showing all detected objects."]},{"cell_type":"code","metadata":{"id":"BK8ZQiikWpYz","executionInfo":{"status":"aborted","timestamp":1608014903123,"user_tz":-60,"elapsed":4077,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["\r\n","def draw_boxes(filename, v_boxes, v_labels, v_scores):\r\n","\t# load the image\r\n","\tdata = pyplot.imread(filename)\r\n","\t# plot the image\r\n","\tpyplot.imshow(data)\r\n","\t# get the context for drawing boxes\r\n","\tax = pyplot.gca()\r\n","\t# plot each box\r\n","\tfor i in range(len(v_boxes)):\r\n","\t\tbox = v_boxes[i]\r\n","\t\t# get coordinates\r\n","\t\ty1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\r\n","\t\t# calculate width and height of the box\r\n","\t\twidth, height = x2 - x1, y2 - y1\r\n","\t\t# create the shape\r\n","\t\trect = Rectangle((x1, y1), width, height, fill=False, color='white')\r\n","\t\t# draw the box\r\n","\t\tax.add_patch(rect)\r\n","\t\t# draw text and score in top left corner\r\n","\t\tlabel = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\r\n","\t\tpyplot.text(x1, y1, label, color='white')\r\n","\t# show the plot\r\n","\tpyplot.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HcluhDwOeUkn"},"source":["# load yolov3 model"]},{"cell_type":"code","metadata":{"id":"dxqw1FSTefBq","executionInfo":{"status":"aborted","timestamp":1608014903124,"user_tz":-60,"elapsed":4071,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["model = load_model('model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pMdPCj0uejet"},"source":["# define the expected input shape for the model"]},{"cell_type":"code","metadata":{"id":"c3rKNI32erbN","executionInfo":{"status":"aborted","timestamp":1608014903126,"user_tz":-60,"elapsed":4067,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["input_w, input_h = 416, 416"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dnS2STe5W_Mh","executionInfo":{"status":"aborted","timestamp":1608014903127,"user_tz":-60,"elapsed":4061,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["# define our new photo\r\n","#photo_filename = '/content/drive/MyDrive/Colab Notebooks/zebra.jpg'\r\n","photo_filename = '/content/drive/MyDrive/__Master Computational Vision 2020-2021/theory/tutorial yolo/zebra.jpg'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mM9CgSbNoLTX","executionInfo":{"status":"aborted","timestamp":1608014903128,"user_tz":-60,"elapsed":4056,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["# load and prepare image\r\n","image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZqUmZQxMXWNM","executionInfo":{"status":"aborted","timestamp":1608014903130,"user_tz":-60,"elapsed":4052,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["# make prediction\r\n","yhat = model.predict(image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SYshW76JXsgb","executionInfo":{"status":"aborted","timestamp":1608014903131,"user_tz":-60,"elapsed":4047,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["# summarize the shape of the list of arrays\r\n","print([a.shape for a in yhat])\r\n","# define the anchors\r\n","anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\r\n","# define the probability threshold for detected objects\r\n","class_threshold = 0.6\r\n","boxes = list()\r\n","for i in range(len(yhat)):\r\n","\t# decode the output of the network\r\n","\tboxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\r\n","# correct the sizes of the bounding boxes for the shape of the image\r\n","correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\r\n","# suppress non-maximal boxes\r\n","do_nms(boxes, 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6FLQIuj1XtCg","executionInfo":{"status":"aborted","timestamp":1608014903133,"user_tz":-60,"elapsed":4041,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["# define the labels\r\n","labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\r\n","\t\"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\r\n","\t\"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\r\n","\t\"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\r\n","\t\"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\r\n","\t\"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\r\n","\t\"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\r\n","\t\"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\r\n","\t\"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\r\n","\t\"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEUAEzCEXWjZ","executionInfo":{"status":"aborted","timestamp":1608014903135,"user_tz":-60,"elapsed":4036,"user":{"displayName":"Petia Radeva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h9QqlRIv-2uV7PAS6xEm9d288bv7MAQSX7Hu4mM=s64","userId":"13422864148079210484"}}},"source":["# get the details of the detected objects\r\n","v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\r\n","# summarize what we found\r\n","for i in range(len(v_boxes)):\r\n","\tprint(v_labels[i], v_scores[i])\r\n","# draw what we found\r\n","draw_boxes(photo_filename, v_boxes, v_labels, v_scores)"],"execution_count":null,"outputs":[]}]}